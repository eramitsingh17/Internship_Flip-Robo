{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835dd38e",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e69007a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Importing required Exceptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8013ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905318da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    Search_tag=driver.find_element_by_id('twotabsearchtextbox')\n",
    "    search_input=str(input('Type here and I will get you there   '))\n",
    "    Search_tag.send_keys(search_input)\n",
    "    time.sleep(5)\n",
    "    search=driver.find_element_by_id('nav-search-submit-button')\n",
    "    search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7f0060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type here and I will get you there   guitars\n"
     ]
    }
   ],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1f00d",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb031211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for different brands url using xpath\n",
    "brands_url = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']//a\")\n",
    "\n",
    "# We will run a loop to iterate all the url links extracted above and extract the href inside them\n",
    "Brands_Url = []\n",
    "for i in range(1,5):\n",
    "    try:\n",
    "        for i in brands_url:\n",
    "            Brands_Url.append(i.get_attribute(\"href\"))\n",
    "    except:\n",
    "        Brands_Url.append(\"-\")\n",
    "              \n",
    "Brands = []\n",
    "Product_Name = []\n",
    "Price = []\n",
    "Exchange = []\n",
    "Ex_Delivery = []\n",
    "Available = []\n",
    "Details = []\n",
    "\n",
    "# Let's traverse all URL one by one\n",
    "for i in Brands_Url:\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(5)\n",
    "       \n",
    "    # Finding the elements for Brand names using id\n",
    "    try: \n",
    "        Brands.append(driver.find_element_by_id(\"productTitle\").text.split(' ')[0]) \n",
    "    except: \n",
    "        Brands.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for product names using id\n",
    "    try: \n",
    "        string=''\n",
    "        for i in driver.find_element_by_id(\"productTitle\").text.split(' ')[1:]:\n",
    "            string=string+' '+i\n",
    "        Product_Name.append(string)\n",
    "    except:\n",
    "        Product_Name.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    \n",
    "    # Finding the elements for prices using id\n",
    "    try: \n",
    "        try: \n",
    "            Price.append(driver.find_element_by_id(\"priceblock_saleprice\").text) \n",
    "        except:\n",
    "            try: \n",
    "                Price.append(driver.find_element_by_id(\"priceblock_dealprice\").text)\n",
    "            except: \n",
    "                Price.append(driver.find_element_by_id(\"priceblock_ourprice\").text)\n",
    "    except: \n",
    "        Price.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for return/exchange using xpath\n",
    "    try: \n",
    "        exchange = driver.find_element_by_xpath(\"//div[@class='a-row icon-farm-wrapper']\").find_elements_by_xpath(\".//div\")\n",
    "        found=False\n",
    "        for i in exchange:\n",
    "            if(i.get_attribute('data-name')=='RETURNS_POLICY'):\n",
    "                found=True\n",
    "                Exchange.append(i.find_element_by_xpath(\".//span[1]/div[2]/a\").text)\n",
    "        if(found==False):\n",
    "            Exchange.append('-')\n",
    "    except:\n",
    "        Exchange.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for expected delivery using id\n",
    "    try: \n",
    "        Ex_Delivery.append(driver.find_element_by_id(\"ddmDeliveryMessage\").find_element_by_xpath(\".//b\").text)\n",
    "    except: \n",
    "        Ex_Delivery.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for availability using id\n",
    "    try: \n",
    "        try: \n",
    "            Available.append(driver.find_element_by_id(\"availability\").find_element_by_xpath(\".//span\").text)\n",
    "        except:\n",
    "            Available.append(driver.find_element_by_id(\"deal_availability\").find_element_by_xpath(\".//div/span\").text)\n",
    "    except: \n",
    "        Available.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for details using id\n",
    "    try:\n",
    "        details = [i.text.replace('\\n','---') for i in driver.find_element_by_id(\"productDetails_techSpec_section_1\").find_elements_by_xpath(\".//tbody\")] \n",
    "        Details.append(details[0])\n",
    "    except: \n",
    "        Details.append('-')\n",
    "    driver.implicitly_wait(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85beebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND NAME</th>\n",
       "      <th>NAME OF THE PRODUCT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>RETURN/EXCHANGE</th>\n",
       "      <th>EXPECTED DELIVERY</th>\n",
       "      <th>AVAILABLILITY</th>\n",
       "      <th>OTHER DETAILS</th>\n",
       "      <th>PRODUCT URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [BRAND NAME, NAME OF THE PRODUCT, PRICE, RETURN/EXCHANGE, EXPECTED DELIVERY, AVAILABLILITY, OTHER DETAILS, PRODUCT URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "ques_2 = pd.DataFrame({})\n",
    "ques_2[\"BRAND NAME\"] = Brands\n",
    "ques_2[\"NAME OF THE PRODUCT\"] = Product_Name\n",
    "ques_2[\"PRICE\"] = Price\n",
    "ques_2[\"RETURN/EXCHANGE\"] = Exchange\n",
    "ques_2[\"EXPECTED DELIVERY\"] = Ex_Delivery\n",
    "ques_2[\"AVAILABLILITY\"] = Available\n",
    "ques_2[\"OTHER DETAILS\"] = Details\n",
    "ques_2[\"PRODUCT URL\"] = Brands_Url\n",
    "ques_2.index+=1\n",
    "\n",
    "# Details of each product in first 3 pages\n",
    "ques_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85df0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_2.to_csv(\"product_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94f384",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0929ea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 300 images\n",
      "Downloading 1 of 300 images\n",
      "Downloading 2 of 300 images\n",
      "Downloading 3 of 300 images\n",
      "Downloading 4 of 300 images\n",
      "Downloading 5 of 300 images\n",
      "Downloading 6 of 300 images\n",
      "Downloading 7 of 300 images\n",
      "Downloading 8 of 300 images\n",
      "Downloading 9 of 300 images\n",
      "Downloading 10 of 300 images\n",
      "Downloading 11 of 300 images\n",
      "Downloading 12 of 300 images\n",
      "Downloading 13 of 300 images\n",
      "Downloading 14 of 300 images\n",
      "Downloading 15 of 300 images\n",
      "Downloading 16 of 300 images\n",
      "Downloading 17 of 300 images\n",
      "Downloading 18 of 300 images\n",
      "Downloading 19 of 300 images\n",
      "Downloading 20 of 300 images\n",
      "Downloading 21 of 300 images\n",
      "Downloading 22 of 300 images\n",
      "Downloading 23 of 300 images\n",
      "Downloading 24 of 300 images\n",
      "Downloading 25 of 300 images\n",
      "Downloading 26 of 300 images\n",
      "Downloading 27 of 300 images\n",
      "Downloading 28 of 300 images\n",
      "Downloading 29 of 300 images\n",
      "Downloading 30 of 300 images\n",
      "Downloading 31 of 300 images\n",
      "Downloading 32 of 300 images\n",
      "Downloading 33 of 300 images\n",
      "Downloading 34 of 300 images\n",
      "Downloading 35 of 300 images\n",
      "Downloading 36 of 300 images\n",
      "Downloading 37 of 300 images\n",
      "Downloading 38 of 300 images\n",
      "Downloading 39 of 300 images\n",
      "Downloading 40 of 300 images\n",
      "Downloading 41 of 300 images\n",
      "Downloading 42 of 300 images\n",
      "Downloading 43 of 300 images\n",
      "Downloading 44 of 300 images\n",
      "Downloading 45 of 300 images\n",
      "Downloading 46 of 300 images\n",
      "Downloading 47 of 300 images\n",
      "Downloading 48 of 300 images\n",
      "Downloading 49 of 300 images\n",
      "Downloading 50 of 300 images\n",
      "Downloading 51 of 300 images\n",
      "Downloading 52 of 300 images\n",
      "Downloading 53 of 300 images\n",
      "Downloading 54 of 300 images\n",
      "Downloading 55 of 300 images\n",
      "Downloading 56 of 300 images\n",
      "Downloading 57 of 300 images\n",
      "Downloading 58 of 300 images\n",
      "Downloading 59 of 300 images\n",
      "Downloading 60 of 300 images\n",
      "Downloading 61 of 300 images\n",
      "Downloading 62 of 300 images\n",
      "Downloading 63 of 300 images\n",
      "Downloading 64 of 300 images\n",
      "Downloading 65 of 300 images\n",
      "Downloading 66 of 300 images\n",
      "Downloading 67 of 300 images\n",
      "Downloading 68 of 300 images\n",
      "Downloading 69 of 300 images\n",
      "Downloading 70 of 300 images\n",
      "Downloading 71 of 300 images\n",
      "Downloading 72 of 300 images\n",
      "Downloading 73 of 300 images\n",
      "Downloading 74 of 300 images\n",
      "Downloading 75 of 300 images\n",
      "Downloading 76 of 300 images\n",
      "Downloading 77 of 300 images\n",
      "Downloading 78 of 300 images\n",
      "Downloading 79 of 300 images\n",
      "Downloading 80 of 300 images\n",
      "Downloading 81 of 300 images\n",
      "Downloading 82 of 300 images\n",
      "Downloading 83 of 300 images\n",
      "Downloading 84 of 300 images\n",
      "Downloading 85 of 300 images\n",
      "Downloading 86 of 300 images\n",
      "Downloading 87 of 300 images\n",
      "Downloading 88 of 300 images\n",
      "Downloading 89 of 300 images\n",
      "Downloading 90 of 300 images\n",
      "Downloading 91 of 300 images\n",
      "Downloading 92 of 300 images\n",
      "Downloading 93 of 300 images\n",
      "Downloading 94 of 300 images\n",
      "Downloading 95 of 300 images\n",
      "Downloading 96 of 300 images\n",
      "Downloading 97 of 300 images\n",
      "Downloading 98 of 300 images\n",
      "Downloading 99 of 300 images\n",
      "Downloading 100 of 300 images\n",
      "Downloading 101 of 300 images\n",
      "Downloading 102 of 300 images\n",
      "Downloading 103 of 300 images\n",
      "Downloading 104 of 300 images\n",
      "Downloading 105 of 300 images\n",
      "Downloading 106 of 300 images\n",
      "Downloading 107 of 300 images\n",
      "Downloading 108 of 300 images\n",
      "Downloading 109 of 300 images\n",
      "Downloading 110 of 300 images\n",
      "Downloading 111 of 300 images\n",
      "Downloading 112 of 300 images\n",
      "Downloading 113 of 300 images\n",
      "Downloading 114 of 300 images\n",
      "Downloading 115 of 300 images\n",
      "Downloading 116 of 300 images\n",
      "Downloading 117 of 300 images\n",
      "Downloading 118 of 300 images\n",
      "Downloading 119 of 300 images\n",
      "Downloading 120 of 300 images\n",
      "Downloading 121 of 300 images\n",
      "Downloading 122 of 300 images\n",
      "Downloading 123 of 300 images\n",
      "Downloading 124 of 300 images\n",
      "Downloading 125 of 300 images\n",
      "Downloading 126 of 300 images\n",
      "Downloading 127 of 300 images\n",
      "Downloading 128 of 300 images\n",
      "Downloading 129 of 300 images\n",
      "Downloading 130 of 300 images\n",
      "Downloading 131 of 300 images\n",
      "Downloading 132 of 300 images\n",
      "Downloading 133 of 300 images\n",
      "Downloading 134 of 300 images\n",
      "Downloading 135 of 300 images\n",
      "Downloading 136 of 300 images\n",
      "Downloading 137 of 300 images\n",
      "Downloading 138 of 300 images\n",
      "Downloading 139 of 300 images\n",
      "Downloading 140 of 300 images\n",
      "Downloading 141 of 300 images\n",
      "Downloading 142 of 300 images\n",
      "Downloading 143 of 300 images\n",
      "Downloading 144 of 300 images\n",
      "Downloading 145 of 300 images\n",
      "Downloading 146 of 300 images\n",
      "Downloading 147 of 300 images\n",
      "Downloading 148 of 300 images\n",
      "Downloading 149 of 300 images\n",
      "Downloading 150 of 300 images\n",
      "Downloading 151 of 300 images\n",
      "Downloading 152 of 300 images\n",
      "Downloading 153 of 300 images\n",
      "Downloading 154 of 300 images\n",
      "Downloading 155 of 300 images\n",
      "Downloading 156 of 300 images\n",
      "Downloading 157 of 300 images\n",
      "Downloading 158 of 300 images\n",
      "Downloading 159 of 300 images\n",
      "Downloading 160 of 300 images\n",
      "Downloading 161 of 300 images\n",
      "Downloading 162 of 300 images\n",
      "Downloading 163 of 300 images\n",
      "Downloading 164 of 300 images\n",
      "Downloading 165 of 300 images\n",
      "Downloading 166 of 300 images\n",
      "Downloading 167 of 300 images\n",
      "Downloading 168 of 300 images\n",
      "Downloading 169 of 300 images\n",
      "Downloading 170 of 300 images\n",
      "Downloading 171 of 300 images\n",
      "Downloading 172 of 300 images\n",
      "Downloading 173 of 300 images\n",
      "Downloading 174 of 300 images\n",
      "Downloading 175 of 300 images\n",
      "Downloading 176 of 300 images\n",
      "Downloading 177 of 300 images\n",
      "Downloading 178 of 300 images\n",
      "Downloading 179 of 300 images\n",
      "Downloading 180 of 300 images\n",
      "Downloading 181 of 300 images\n",
      "Downloading 182 of 300 images\n",
      "Downloading 183 of 300 images\n",
      "Downloading 184 of 300 images\n",
      "Downloading 185 of 300 images\n",
      "Downloading 186 of 300 images\n",
      "Downloading 187 of 300 images\n",
      "Downloading 188 of 300 images\n",
      "Downloading 189 of 300 images\n",
      "Downloading 190 of 300 images\n",
      "Downloading 191 of 300 images\n",
      "Downloading 192 of 300 images\n",
      "Downloading 193 of 300 images\n",
      "Downloading 194 of 300 images\n",
      "Downloading 195 of 300 images\n",
      "Downloading 196 of 300 images\n",
      "Downloading 197 of 300 images\n",
      "Downloading 198 of 300 images\n",
      "Downloading 199 of 300 images\n",
      "Downloading 200 of 300 images\n",
      "Downloading 201 of 300 images\n",
      "Downloading 202 of 300 images\n",
      "Downloading 203 of 300 images\n",
      "Downloading 204 of 300 images\n",
      "Downloading 205 of 300 images\n",
      "Downloading 206 of 300 images\n",
      "Downloading 207 of 300 images\n",
      "Downloading 208 of 300 images\n",
      "Downloading 209 of 300 images\n",
      "Downloading 210 of 300 images\n",
      "Downloading 211 of 300 images\n",
      "Downloading 212 of 300 images\n",
      "Downloading 213 of 300 images\n",
      "Downloading 214 of 300 images\n",
      "Downloading 215 of 300 images\n",
      "Downloading 216 of 300 images\n",
      "Downloading 217 of 300 images\n",
      "Downloading 218 of 300 images\n",
      "Downloading 219 of 300 images\n",
      "Downloading 220 of 300 images\n",
      "Downloading 221 of 300 images\n",
      "Downloading 222 of 300 images\n",
      "Downloading 223 of 300 images\n",
      "Downloading 224 of 300 images\n",
      "Downloading 225 of 300 images\n",
      "Downloading 226 of 300 images\n",
      "Downloading 227 of 300 images\n",
      "Downloading 228 of 300 images\n",
      "Downloading 229 of 300 images\n",
      "Downloading 230 of 300 images\n",
      "Downloading 231 of 300 images\n",
      "Downloading 232 of 300 images\n",
      "Downloading 233 of 300 images\n",
      "Downloading 234 of 300 images\n",
      "Downloading 235 of 300 images\n",
      "Downloading 236 of 300 images\n",
      "Downloading 237 of 300 images\n",
      "Downloading 238 of 300 images\n",
      "Downloading 239 of 300 images\n",
      "Downloading 240 of 300 images\n",
      "Downloading 241 of 300 images\n",
      "Downloading 242 of 300 images\n",
      "Downloading 243 of 300 images\n",
      "Downloading 244 of 300 images\n",
      "Downloading 245 of 300 images\n",
      "Downloading 246 of 300 images\n",
      "Downloading 247 of 300 images\n",
      "Downloading 248 of 300 images\n",
      "Downloading 249 of 300 images\n",
      "Downloading 250 of 300 images\n",
      "Downloading 251 of 300 images\n",
      "Downloading 252 of 300 images\n",
      "Downloading 253 of 300 images\n",
      "Downloading 254 of 300 images\n",
      "Downloading 255 of 300 images\n",
      "Downloading 256 of 300 images\n",
      "Downloading 257 of 300 images\n",
      "Downloading 258 of 300 images\n",
      "Downloading 259 of 300 images\n",
      "Downloading 260 of 300 images\n",
      "Downloading 261 of 300 images\n",
      "Downloading 262 of 300 images\n",
      "Downloading 263 of 300 images\n",
      "Downloading 264 of 300 images\n",
      "Downloading 265 of 300 images\n",
      "Downloading 266 of 300 images\n",
      "Downloading 267 of 300 images\n",
      "Downloading 268 of 300 images\n",
      "Downloading 269 of 300 images\n",
      "Downloading 270 of 300 images\n",
      "Downloading 271 of 300 images\n",
      "Downloading 272 of 300 images\n",
      "Downloading 273 of 300 images\n",
      "Downloading 274 of 300 images\n",
      "Downloading 275 of 300 images\n",
      "Downloading 276 of 300 images\n",
      "Downloading 277 of 300 images\n",
      "Downloading 278 of 300 images\n",
      "Downloading 279 of 300 images\n",
      "Downloading 280 of 300 images\n",
      "Downloading 281 of 300 images\n",
      "Downloading 282 of 300 images\n",
      "Downloading 283 of 300 images\n",
      "Downloading 284 of 300 images\n",
      "Downloading 285 of 300 images\n",
      "Downloading 286 of 300 images\n",
      "Downloading 287 of 300 images\n",
      "Downloading 288 of 300 images\n",
      "Downloading 289 of 300 images\n",
      "Downloading 290 of 300 images\n",
      "Downloading 291 of 300 images\n",
      "Downloading 292 of 300 images\n",
      "Downloading 293 of 300 images\n",
      "Downloading 294 of 300 images\n",
      "Downloading 295 of 300 images\n",
      "Downloading 296 of 300 images\n",
      "Downloading 297 of 300 images\n",
      "Downloading 298 of 300 images\n",
      "Downloading 299 of 300 images\n"
     ]
    }
   ],
   "source": [
    "#Activating the chrome browser with specified url\n",
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "# getting images.google.com\n",
    "url = \"https://images.google.com/\"\n",
    "#Creating empty list and giving search items as list and creating loop\n",
    "urls = []    \n",
    "data = []\n",
    "search_item = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "for item in search_item:\n",
    "    driver.get(url)  \n",
    "    time.sleep(5)\n",
    "    search_bar = driver.find_element_by_tag_name(\"input\") #Xpath for search bar\n",
    "    \n",
    "    search_bar.send_keys(str(item))      #sending key word for search item\n",
    "    \n",
    "    search_button =driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\").click() #Clicking on search button\n",
    "    \n",
    "    # scrolling the web page to get more images\n",
    "    for _ in range(500):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "        \n",
    "        imgs = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "                if(source[0:4] == 'http'):\n",
    "                    img_url.append(source)\n",
    "    for i in img_url[:100]:\n",
    "        urls.append(i)\n",
    "                    \n",
    "for i in range(len(urls)):\n",
    "    if i >= 300:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 300))\n",
    "    response = requests.get(urls[i])\n",
    "\n",
    "    file = open(r\"E:\\Flip Robo Internship\\Assignments\\Web Scrapping Assignment\\Web Scraping Assignment-3\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1fe458",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "21821174",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.flipkart.com/'\n",
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "52b19216",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_product = driver.find_element_by_xpath('//input[@class=\"_3704LK\"]')\n",
    "search_btn= driver.find_element_by_xpath('//button[@class =\"L0Z3Pu\"]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "28911fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_product.send_keys('Oneplus Nord, pixel 4A')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3f2b75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Url=[]\n",
    "url = driver.find_elements_by_xpath('//a[@class =\"_1fQZEK\"]')\n",
    "for i in url:\n",
    "    Url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "08ff6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_name =[]\n",
    "Price=[]\n",
    "Colour=[]\n",
    "Display_size=[]\n",
    "Display_resolution=[]\n",
    "Primary_Camera=[]\n",
    "Secondary_camera=[]\n",
    "RAM=[]\n",
    "ROM= []\n",
    "Battery_capacity =[]\n",
    "Processor =[]\n",
    "Processor_core =[]\n",
    "\n",
    "for i in Url:\n",
    "    driver.get(i)\n",
    "    sleep(2)\n",
    "    \n",
    "    readmore =driver.find_element_by_xpath('//button[@class =\"_2KpZ6l _1FH0tX\"]')\n",
    "    readmore.click()\n",
    "    sleep(2)\n",
    "\n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')\n",
    "        Brand_name.append(brand.text)\n",
    "        \n",
    "    except:\n",
    "        Brand_name.append('--') \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        Price.append(price.text)\n",
    "        \n",
    "    except:\n",
    "        Price.append('--')\n",
    "        \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Colour.append(color.text)\n",
    "        \n",
    "    except:\n",
    "        Colour.append('--')\n",
    "        \n",
    "    try:\n",
    "        dsize = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_size.append(dsize.text)\n",
    "        \n",
    "    except:\n",
    "        Display_size.append('--')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        resolution = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][2]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Display_resolution.append(resolution.text)\n",
    "        \n",
    "    except:\n",
    "        Display_resolution.append('--')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        pcamera = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Primary_Camera.append(pcamera.text)\n",
    "        \n",
    "    except:\n",
    "        Primary_Camera.append('--')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        scamera = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][5]/table/tbody/tr[5]/td[2]/ul/li')\n",
    "        Secondary_camera.append(scamera.text)\n",
    "        \n",
    "    except:\n",
    "        Secondary_camera.append('--')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        RAM.append(ram.text)\n",
    "        \n",
    "    except:\n",
    "        RAM.append('--')\n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        ROM.append(rom.text)\n",
    "        \n",
    "    except:\n",
    "        ROM.append('--')\n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        battery = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][10]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Battery_capacity.append(battery.text)\n",
    "        \n",
    "    except:\n",
    "        Battery_capacity.append('--')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        processor = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][3]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Processor.append(processor.text)\n",
    "        \n",
    "    except:\n",
    "        Processor.append('--')\n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        core = driver.find_element_by_xpath('//div [@class =\"_3k-BhJ\"][3]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Processor_core.append(core.text)\n",
    "        \n",
    "    except:\n",
    "        Processor_core.append('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "58c353c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand_name / Mobile name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Display_size</th>\n",
       "      <th>Display_resolution</th>\n",
       "      <th>Primary_Camera</th>\n",
       "      <th>Secondary_camera</th>\n",
       "      <th>RAM</th>\n",
       "      <th>ROM</th>\n",
       "      <th>Battery_capacity</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Processor_core</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand_name / Mobile name, Price, Colour, Display_size, Display_resolution, Primary_Camera, Secondary_camera, RAM, ROM, Battery_capacity, Processor, Processor_core, Product URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smartphone_data = pd.DataFrame({})\n",
    "\n",
    "Smartphone_data['Brand_name / Mobile name'] =Brand_name\n",
    "Smartphone_data['Price'] =Price\n",
    "Smartphone_data['Colour'] =Colour\n",
    "Smartphone_data['Display_size'] =Display_size\n",
    "Smartphone_data['Display_resolution'] =Display_resolution\n",
    "Smartphone_data['Primary_Camera'] =Primary_Camera\n",
    "Smartphone_data['Secondary_camera'] =Secondary_camera\n",
    "Smartphone_data['RAM'] =RAM\n",
    "Smartphone_data['ROM'] =ROM\n",
    "Smartphone_data['Battery_capacity'] =Battery_capacity \n",
    "Smartphone_data['Processor'] =Processor \n",
    "Smartphone_data['Processor_core'] =Processor_core \n",
    "Smartphone_data['Product URL'] = Url\n",
    "\n",
    "Smartphone_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c19978",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on googlemaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8234a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "# Get the webpage\n",
    "url=\"https://www.google.co.in/maps/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "463423f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_city=driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "search_city.send_keys('Panipat Haryana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f716c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver.find_element_by_xpath(\"//button[@id='searchbox-searchbutton']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36306551",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate=driver.find_element_by_xpath(\"//div[@class='gb_1f gb_h']/div/a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "757cba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=coordinate.get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cb2f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "cordinates= re.findall('\\d\\d\\.\\d+', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6150b13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29.383028',\n",
       " '76.9371419',\n",
       " '13.56',\n",
       " '29.3909464',\n",
       " '76.9635023',\n",
       " '29.3909464',\n",
       " '76.9635023']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78e48f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Longitutde =cordinates[0:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a9af5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29.383028', '13.56']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Longitutde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81ba8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude=cordinates[1:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c4ff7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['76.9371419', '29.3909464']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b3ad286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitutde</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.383028</td>\n",
       "      <td>76.9371419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.56</td>\n",
       "      <td>29.3909464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Longitutde    latitude\n",
       "0  29.383028  76.9371419\n",
       "1      13.56  29.3909464"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps=pd.DataFrame({})\n",
    "maps['Longitutde']=Longitutde\n",
    "maps['latitude']=latitude\n",
    "maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975520e",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9f526c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "url='https://trak.in/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b188a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_tab = driver.find_element_by_xpath('//li[@id =\"menu-item-51510\"]')\n",
    "funding_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea8b1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date =[]\n",
    "Startup_name =[]\n",
    "Industry_vertical =[]\n",
    "Sub_vertical =[]\n",
    "City_location=[]\n",
    "Investers_name=[]\n",
    "Investment_type =[]\n",
    "Amount =[]\n",
    "\n",
    "\n",
    "\n",
    "dateJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[2]')\n",
    "dateAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[2]')\n",
    "datesep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[2]')\n",
    "\n",
    "for i in [dateJly,dateAug,datesep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Date.append(j.text)\n",
    "    except:\n",
    "        Date.append('--')\n",
    "\n",
    "startupJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[3]')\n",
    "startupAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[3]')\n",
    "startupsep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[3]')\n",
    "\n",
    "for i in [startupJly,startupAug,startupsep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Startup_name.append(j.text)\n",
    "    except:\n",
    "        Startup_name.append('--')\n",
    "\n",
    "IndustryJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[4]')\n",
    "IndustryAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[4]')\n",
    "Industrysep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[4]')\n",
    "\n",
    "for i in [IndustryJly,IndustryAug,Industrysep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Industry_vertical.append(j.text)\n",
    "    except:\n",
    "        Industry_vertical.append('--')\n",
    "\n",
    "Sub_verticalJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[5]')\n",
    "Sub_verticalAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[5]')\n",
    "Sub_verticalsep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[5]')\n",
    "\n",
    "for i in [Sub_verticalJly,Sub_verticalAug,Sub_verticalsep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Sub_vertical.append(j.text)\n",
    "    except:\n",
    "        Sub_vertical.append('--')\n",
    "\n",
    "CityJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[6]')\n",
    "CityAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[6]')\n",
    "Citysep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[6]')\n",
    "\n",
    "for i in [CityJly,CityAug,Citysep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            City_location.append(j.text)\n",
    "    except:\n",
    "        City_location.append('--')\n",
    "\n",
    "InvestersJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[7]')\n",
    "InvestersAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[7]')\n",
    "Investerssep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[7]')\n",
    "\n",
    "for i in [InvestersJly,InvestersAug,Investerssep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Investers_name.append(j.text)\n",
    "    except:\n",
    "        Investers_name.append('--')\n",
    "\n",
    "Iv_typeJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[8]')\n",
    "Iv_typeAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[8]')\n",
    "Iv_typesep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[8]')\n",
    "\n",
    "for i in [Iv_typeJly,Iv_typeAug,Iv_typesep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Investment_type.append(j.text)\n",
    "    except:\n",
    "        Investment_type.append('--')\n",
    "\n",
    "AmountJly =driver.find_elements_by_xpath('//div[@id =\"tablepress-48_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[9]')\n",
    "AmountAug =driver.find_elements_by_xpath('//div[@id =\"tablepress-49_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[9]')\n",
    "Amountsep =driver.find_elements_by_xpath('//div[@id =\"tablepress-50_wrapper\"]/div[3]/div[2]/table/tbody/tr//td[9]')\n",
    "\n",
    "for i in [AmountJly,AmountAug,Amountsep]:\n",
    "    try:\n",
    "        for j in i:\n",
    "            Amount.append(j.text)\n",
    "    except:\n",
    "        Amount.append('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1433657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup_name</th>\n",
       "      <th>Industry / vertical</th>\n",
       "      <th>Sub_vertical</th>\n",
       "      <th>City / location</th>\n",
       "      <th>Investers'name</th>\n",
       "      <th>Investment_type</th>\n",
       "      <th>Amount in(USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Startup_name, Industry / vertical, Sub_vertical, City / location, Investers'name, Investment_type, Amount in(USD)]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quaterly_funding_dataset =pd.DataFrame({})\n",
    "\n",
    "Quaterly_funding_dataset['Date'] = Date\n",
    "Quaterly_funding_dataset['Startup_name'] = Startup_name \n",
    "Quaterly_funding_dataset['Industry / vertical'] = Industry_vertical \n",
    "Quaterly_funding_dataset['Sub_vertical'] = Sub_vertical \n",
    "Quaterly_funding_dataset['City / location'] = City_location\n",
    "Quaterly_funding_dataset[\"Investers'name\"] = Investers_name\n",
    "Quaterly_funding_dataset['Investment_type'] = Investment_type \n",
    "Quaterly_funding_dataset['Amount in(USD)'] = Amount\n",
    "\n",
    "Quaterly_funding_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f0826",
   "metadata": {},
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "868a24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "# Get the webpage\n",
    "url=\"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95eaa26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Laptop_btn=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[4]/ul/li[3]/a\")\n",
    "Laptop_btn.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "273c4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latest_btn=driver.find_element_by_xpath(\"/html/body/div[6]/div/div[2]/div[2]/ul/li[10]/a\")\n",
    "Latest_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d09af784",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_name=[]\n",
    "OS=[] \n",
    "Display=[]\n",
    "Processor=[]\n",
    "Memory=[]\n",
    "Weight=[]\n",
    "Dimension=[]\n",
    "Graphics_Processor=[]\n",
    "\n",
    "product_name= driver.find_elements_by_xpath(\"//div[@data-cat='Top_Ten_En_Scroll']/a/h3\")\n",
    "for p in product_name:\n",
    "    Product_name.append(p.text)\n",
    "    \n",
    "Os=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[3]/td[3]\")\n",
    "for o in Os:\n",
    "    OS.append(o.text)\n",
    "\n",
    "display=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[4]/td[3]\")\n",
    "for d in display:\n",
    "    Display.append(d.text)\n",
    "\n",
    "processor=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[5]/td[3]\")\n",
    "for pr in processor:\n",
    "    Processor.append(pr.text)\n",
    "    \n",
    "memory =driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[6]/td[3]\")\n",
    "for m in memory:\n",
    "    Memory.append(m.text)\n",
    "    \n",
    "weight =driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[7]/td[3]\")\n",
    "for w in weight:\n",
    "    Weight.append(w.text)\n",
    "    \n",
    "dimension =driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[8]/td[3]\")\n",
    "for d in dimension:\n",
    "    Dimension.append(d.text)\n",
    "\n",
    "graphics_Processor=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[9]/td[3]\")\n",
    "for g in graphics_Processor:\n",
    "    Graphics_Processor.append(g.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38c022a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price=[]\n",
    "price=driver.find_elements_by_xpath(\"//table[@id='summtable']\")\n",
    "for i in price:\n",
    "    Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67f4dc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_name</th>\n",
       "      <th>OS</th>\n",
       "      <th>Display</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Graphics_Processor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACER NITRO 5 RYZEN 9 (2021)</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>15.6\" (1920 x 1080)</td>\n",
       "      <td>1 TB HDD/16 GBGB DDR4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>363.4 x 255 x 23.9</td>\n",
       "      <td>NVIDIA GeForce RTX 3070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI STEALTH 15M 11TH GEN CORE I7-11375H (2021</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>15.6\" (1920 x 1080)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>358.3 x 248 x 16.15</td>\n",
       "      <td>NVIDIA GeForce RTX 3060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15 RYZEN 9-5900HX (2021)</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>15.6\" (2560 x 1440)</td>\n",
       "      <td>2 TB SSD/32 GBGB DDR4</td>\n",
       "      <td>2.30</td>\n",
       "      <td>354 x 259 x 22.6</td>\n",
       "      <td>NVIDIA GeForce RTX 3080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALIENWARE AREA 51M R2</td>\n",
       "      <td>Windows 10 Home</td>\n",
       "      <td>17.3\" (1920 x 1080)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>27.65 x 402.6 x 319.14</td>\n",
       "      <td>Intel® UHD Graphics 630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALIENWARE M15 R3</td>\n",
       "      <td>Windows 10 Home</td>\n",
       "      <td>15.6\" (3840 x 2160)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>Windows 10 Home</td>\n",
       "      <td>15.6\" (1920 x 1080)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>2.30</td>\n",
       "      <td>35.4 x 25.9 x 2.26</td>\n",
       "      <td>NVIDIA® GeForce RTX™ 3070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS ROG ZEPHYRUS G14</td>\n",
       "      <td>Windows 10 Home</td>\n",
       "      <td>14\" (1920 x 1080)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>1.65</td>\n",
       "      <td>32.5 x 22.1 x 1.8</td>\n",
       "      <td>NVIDIA GeForce RTX 2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LENOVO LEGION 5I</td>\n",
       "      <td>Windows 10 Pro</td>\n",
       "      <td>15.6\" (1920 x 1080)</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>363.06 x 259.61 x 23.57</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650 4GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS ROG ZEPHYRUS DUO 15</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>15.6\" (3840 x 1100)</td>\n",
       "      <td>512 GB SSD/4 GBGB DDR4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>268.30 x 360.00 x 20.90</td>\n",
       "      <td>NVIDIA GeForce RTX 2070 Max-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACER ASPIRE 7 GAMING</td>\n",
       "      <td>Windows 10 Home</td>\n",
       "      <td>15.6\" (1920 x 1080)</td>\n",
       "      <td>512 GB SSD/8 GBGB DDR4</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.29 x 36.3 x 25.4</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Product_name               OS  \\\n",
       "0                    ACER NITRO 5 RYZEN 9 (2021)       Windows 10   \n",
       "1  MSI STEALTH 15M 11TH GEN CORE I7-11375H (2021       Windows 10   \n",
       "2   ASUS ROG STRIX SCAR 15 RYZEN 9-5900HX (2021)       Windows 10   \n",
       "3                          ALIENWARE AREA 51M R2  Windows 10 Home   \n",
       "4                               ALIENWARE M15 R3  Windows 10 Home   \n",
       "5                         ASUS ROG STRIX SCAR 15  Windows 10 Home   \n",
       "6                          ASUS ROG ZEPHYRUS G14  Windows 10 Home   \n",
       "7                               LENOVO LEGION 5I   Windows 10 Pro   \n",
       "8                       ASUS ROG ZEPHYRUS DUO 15       Windows 10   \n",
       "9                           ACER ASPIRE 7 GAMING  Windows 10 Home   \n",
       "\n",
       "               Display                  Memory Weight  \\\n",
       "0  15.6\" (1920 x 1080)   1 TB HDD/16 GBGB DDR4    2.4   \n",
       "1  15.6\" (1920 x 1080)   1 TB SSD/16 GBGB DDR4    1.7   \n",
       "2  15.6\" (2560 x 1440)   2 TB SSD/32 GBGB DDR4   2.30   \n",
       "3  17.3\" (1920 x 1080)   1 TB SSD/16 GBGB DDR4    4.1   \n",
       "4  15.6\" (3840 x 2160)   1 TB SSD/16 GBGB DDR4     NA   \n",
       "5  15.6\" (1920 x 1080)   1 TB SSD/16 GBGB DDR4   2.30   \n",
       "6    14\" (1920 x 1080)   1 TB SSD/16 GBGB DDR4   1.65   \n",
       "7  15.6\" (1920 x 1080)   1 TB SSD/16 GBGB DDR4    2.3   \n",
       "8  15.6\" (3840 x 1100)  512 GB SSD/4 GBGB DDR4    2.4   \n",
       "9  15.6\" (1920 x 1080)  512 GB SSD/8 GBGB DDR4   2.15   \n",
       "\n",
       "                 Dimension             Graphics_Processor  \n",
       "0       363.4 x 255 x 23.9        NVIDIA GeForce RTX 3070  \n",
       "1      358.3 x 248 x 16.15        NVIDIA GeForce RTX 3060  \n",
       "2         354 x 259 x 22.6        NVIDIA GeForce RTX 3080  \n",
       "3   27.65 x 402.6 x 319.14        Intel® UHD Graphics 630  \n",
       "4                       NA                             NA  \n",
       "5       35.4 x 25.9 x 2.26      NVIDIA® GeForce RTX™ 3070  \n",
       "6        32.5 x 22.1 x 1.8        NVIDIA GeForce RTX 2060  \n",
       "7  363.06 x 259.61 x 23.57  NVIDIA® GeForce® GTX 1650 4GB  \n",
       "8  268.30 x 360.00 x 20.90  NVIDIA GeForce RTX 2070 Max-Q  \n",
       "9       2.29 x 36.3 x 25.4      NVIDIA® GeForce® GTX 1650  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Laptops=pd.DataFrame({})\n",
    "Laptops['Product_name']=Product_name\n",
    "Laptops['OS']=OS\n",
    "Laptops['Display']=Display\n",
    "Laptops['Memory']=Memory\n",
    "Laptops['Weight']=Weight\n",
    "Laptops['Dimension']=Dimension\n",
    "Laptops['Graphics_Processor']=Graphics_Processor\n",
    "Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b447e58",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "662d3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "# Get the webpage\n",
    "url='https://www.forbes.com/billionaires/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e767e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n",
      "Scraping Page 2\n",
      "Scraping Page 3\n",
      "Scraping Page 4\n",
      "Scraping Page 5\n",
      "Scraping Page 6\n",
      "Scraping Page 7\n",
      "Scraping Page 8\n",
      "Scraping Page 9\n",
      "Scraping Page 10\n",
      "Scraping Page 11\n",
      "Scraping Page 12\n",
      "Scraping Page 13\n",
      "Scraping Page 14\n"
     ]
    }
   ],
   "source": [
    "Rank=[]\n",
    "Name=[]\n",
    "Net_worth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry =[]\n",
    "for i in range(14):\n",
    "    print('Scraping Page',i+1)\n",
    "    \n",
    "    rank=driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for r in rank:\n",
    "        Rank.append(r.text)  \n",
    "    time.sleep(1) \n",
    "    \n",
    "    name=driver.find_elements_by_xpath(\"//div[@class='personName']\")\n",
    "    for n in name:\n",
    "        Name.append(n.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    net_worth=driver.find_elements_by_xpath(\"//div[@class='netWorth']\")\n",
    "    for nw in net_worth:\n",
    "        Net_worth.append(nw.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    age=driver.find_elements_by_xpath(\"//div[@class='age']\")\n",
    "    for a in age:\n",
    "        Age.append(a.text)\n",
    "    time.sleep(1)\n",
    "        \n",
    "    citizenship=driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for c in citizenship:\n",
    "        Citizenship.append(c.text)\n",
    "    time.sleep(1)\n",
    "        \n",
    "    source=driver.find_elements_by_xpath(\"//div[@class='source-column']\")\n",
    "    for s in source:\n",
    "        Source.append(s.text)\n",
    "    time.sleep(1)\n",
    "        \n",
    "    industry=driver.find_elements_by_xpath(\"//div[@class='category']\")\n",
    "    for i in industry:\n",
    "        Industry.append(i.text)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        Next_btn=driver.find_element_by_xpath(\"//div[@class='next-page']\")\n",
    "        Next_btn.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        break             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb2e5a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net_worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$177 B</td>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$151 B</td>\n",
       "      <td>49</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$150 B</td>\n",
       "      <td>72</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$124 B</td>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>$97 B</td>\n",
       "      <td>36</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Daniel Yong Zhang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>49</td>\n",
       "      <td>China</td>\n",
       "      <td>e-commerce</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhang Yuqiang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>65</td>\n",
       "      <td>China</td>\n",
       "      <td>Fiberglass</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhao Meiguang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>gold mining</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhong Naixiong</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>conglomerate</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhou Wei family</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>54</td>\n",
       "      <td>China</td>\n",
       "      <td>Software</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank                      Name Net_worth Age    Citizenship  \\\n",
       "0        1.                Jeff Bezos    $177 B  57  United States   \n",
       "1        2.                 Elon Musk    $151 B  49  United States   \n",
       "2        3.  Bernard Arnault & family    $150 B  72         France   \n",
       "3        4.                Bill Gates    $124 B  65  United States   \n",
       "4        5.           Mark Zuckerberg     $97 B  36  United States   \n",
       "...     ...                       ...       ...  ..            ...   \n",
       "2750  2674.         Daniel Yong Zhang      $1 B  49          China   \n",
       "2751  2674.             Zhang Yuqiang      $1 B  65          China   \n",
       "2752  2674.             Zhao Meiguang      $1 B  58          China   \n",
       "2753  2674.            Zhong Naixiong      $1 B  58          China   \n",
       "2754  2674.           Zhou Wei family      $1 B  54          China   \n",
       "\n",
       "             Source          Industry  \n",
       "0            Amazon        Technology  \n",
       "1     Tesla, SpaceX        Automotive  \n",
       "2              LVMH  Fashion & Retail  \n",
       "3         Microsoft        Technology  \n",
       "4          Facebook        Technology  \n",
       "...             ...               ...  \n",
       "2750     e-commerce        Technology  \n",
       "2751     Fiberglass     Manufacturing  \n",
       "2752    gold mining   Metals & Mining  \n",
       "2753   conglomerate       Diversified  \n",
       "2754       Software        Technology  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Billionaires=pd.DataFrame({})\n",
    "Billionaires['Rank']=Rank\n",
    "Billionaires['Name']= Name\n",
    "Billionaires['Net_worth']=Net_worth\n",
    "Billionaires['Age']=Age\n",
    "Billionaires['Citizenship']=Citizenship\n",
    "Billionaires['Source']=Source\n",
    "Billionaires['Industry']=Industry\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530aad35",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e1c8b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "# Opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "i=0\n",
    "while(i<100):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\") # scroll down to get more comments\n",
    "    i+=1\n",
    "while(i<402):\n",
    "    driver.execute_script(\"window.scrollBy(0,5000)\") # scroll down to get more comments\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6eb79db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = []\n",
    "upvt = []\n",
    "cmttime = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "08bfd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_tag = driver.find_elements_by_xpath(\"//*[@id='content-text']\")\n",
    "for i in cmt_tag:\n",
    "    cmt.append(i.text.replace(\"\\n\",''))\n",
    "\n",
    "upvt_tag = driver.find_elements_by_xpath(\"//*[@id='vote-count-middle']\")\n",
    "for j in upvt_tag:\n",
    "    try: \n",
    "        upvt.append(j.text)\n",
    "    except:\n",
    "        upvt.append('-')\n",
    "cmttime_tag = driver.find_elements_by_xpath(\"//*[@id='header-author']/yt-formatted-string/a\")\n",
    "for k in cmttime_tag:\n",
    "    try: \n",
    "        cmttime.append(k.text)\n",
    "    except:\n",
    "        cmttime.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1fbd663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(upvt)):\n",
    "    if upvt[i] == '': \n",
    "        upvt[i] = '-' # replacing empty string with '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c45672b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Youtube = {'Comment':cmt,'Commented':cmttime,'Number of upvotes':upvt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c576c4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Commented</th>\n",
       "      <th>Number of upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Comment, Commented, Number of upvotes]\n",
       "Index: []"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YoutubeDF = pd.DataFrame(Youtube)\n",
    "YoutubeDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837eec0",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "339c4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'E:/Flip Robo Internship/chromedriver_win32/chromedriver.exe')\n",
    "url=\"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f0c42eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_hotel=driver.find_element_by_xpath(\"//input[@id='search-input-field']\")\n",
    "search_hotel.click()\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2e1ea527",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_hotel1=driver.find_element_by_xpath(\"//input[@id='search-input-field']\")\n",
    "search_hotel1.send_keys('London')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa418390",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn1=driver.find_element_by_xpath(\"//div[@class='label']\")\n",
    "search_btn1.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "df51f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@id='search-button']\")\n",
    "search_btn.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c1f20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n"
     ]
    }
   ],
   "source": [
    "Hotel_Name=[]\n",
    "Dist_from_city=[]\n",
    "Total_reviews=[]\n",
    "Overall_reviews=[]\n",
    "Facility=[]\n",
    "Page_url=[]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print('Scraping Page',i+1)\n",
    "    \n",
    "    hotel_name=driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "    for h in hotel_name:\n",
    "        Hotel_Name.append(h.text)\n",
    "        \n",
    "    page_url=driver.find_elements_by_xpath(\"//h2[@class='title title-6']/a\")\n",
    "    for r in page_url:\n",
    "        Page_url.append(r.get_attribute('href'))\n",
    "    \n",
    "    dist_from_city=driver.find_elements_by_xpath(\"//span[@class='description']\")\n",
    "    for dist in dist_from_city:\n",
    "        Dist_from_city.append(dist.text)\n",
    "    \n",
    "    total_reviews=driver.find_elements_by_xpath(\"//div[@class='reviews']\")\n",
    "    for t in total_reviews:\n",
    "        Total_reviews.append(t.text.replace('Total Reviews','').strip())\n",
    "        \n",
    "    overall_reviews=driver.find_elements_by_xpath(\"//div[@class='keyword']\")\n",
    "    for t in overall_reviews:\n",
    "        Overall_reviews.append(t.text)\n",
    "        \n",
    "  \n",
    "    facility=driver.find_elements_by_xpath(\"//div[@class='facilities-label facilities']\")\n",
    "    for t in facility:\n",
    "        Facility.append(t.text.replace('\\n','--'))\n",
    "    try:\n",
    "        Next_btn=driver.find_element_by_xpath(\"//div[@class='pagination-item pagination-next']\")\n",
    "        Next_btn.click() \n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8efa29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratings=[]\n",
    "Property_desc=[]\n",
    "for i in Page_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        property_desc=driver.find_element_by_xpath(\"//div[@class='content collapse-content']\")\n",
    "        Property_desc.append(property_desc.text)\n",
    "    except:\n",
    "        Property_desc.append('--')\n",
    "    \n",
    "    try:\n",
    "        rating=driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        Ratings.append(rating.text)\n",
    "    except:\n",
    "        Ratings.append('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1280d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Dist_from_city</th>\n",
       "      <th>Total_reviews</th>\n",
       "      <th>Overall_reviews</th>\n",
       "      <th>Facility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Hotel_Name, Dist_from_city, Total_reviews, Overall_reviews, Facility]\n",
       "Index: []"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hotel_detail=pd.DataFrame({})\n",
    "Hotel_detail['Hotel_Name']= Hotel_Name\n",
    "Hotel_detail['Dist_from_city']=Dist_from_city\n",
    "Hotel_detail['Total_reviews']=Total_reviews\n",
    "Hotel_detail['Overall_reviews']=Overall_reviews\n",
    "Hotel_detail['Facility']=Facility\n",
    "\n",
    "Hotel_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75836a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86937368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
